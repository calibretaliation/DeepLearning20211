{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese_net_tf.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calibretaliation/DeepLearning20211/blob/main/siamese_net_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NijPZmQCTMkj",
        "outputId": "9455acc9-6f8d-410c-8a3e-df0469c87c0f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shopee matching problem using Siamese-inspired approach"
      ],
      "metadata": {
        "id": "6WmolMpV24w_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "CeDeB7H-A5Oj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "istTBFbsNPcR",
        "outputId": "90e0501b-a663-47f0-b8cd-9470ecb4d46c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepLearning20211'...\n",
            "remote: Enumerating objects: 32787, done.\u001b[K\n",
            "remote: Counting objects: 100% (321/321), done.\u001b[K\n",
            "remote: Compressing objects: 100% (321/321), done.\u001b[K\n",
            "remote: Total 32787 (delta 188), reused 0 (delta 0), pack-reused 32466\u001b[K\n",
            "Receiving objects: 100% (32787/32787), 1.69 GiB | 34.96 MiB/s, done.\n",
            "Resolving deltas: 100% (246/246), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/calibretaliation/DeepLearning20211.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U tensorflow-addons"
      ],
      "metadata": {
        "id": "cUOIsVncQGib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import cm\n",
        "import math\n",
        "import random\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from tensorflow.keras import applications, layers, losses, optimizers, metrics, Model, callbacks\n",
        "from tensorflow.keras.applications import resnet, mobilenet_v3\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import backend\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import top_k_accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# import tensorflow_docs as tfdocs\n",
        "# import tensorflow_docs.modeling\n",
        "# import tensorflow_docs.plots\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "metadata": {
        "id": "WJyjcOIXQTWO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and preprocess datasets"
      ],
      "metadata": {
        "id": "G2uFtS8S6kM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load CSV\n",
        "\n",
        "train_df = pd.read_csv('DeepLearning20211/data/train.csv')\n",
        "train_img_dir = Path('DeepLearning20211/data/train_images')"
      ],
      "metadata": {
        "id": "0NZlAgwOQj9q",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df = pd.concat([train_df.iloc[:100], train_df.iloc[:100]]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Gsk3tRaKsisZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot class cardinality distribution\n",
        "\n",
        "# calculate number of images in each class\n",
        "label_count = train_df['label_group'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "label_count.value_counts().plot.bar()\n",
        "plt.xlabel('Class cardinality')\n",
        "plt.ylabel('Number of class')\n",
        "plt.title('Shopee Class cardinality distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w8c7i4HsoEeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create validation set\n",
        "\n",
        "train_df['index'] = train_df.index\n",
        "val_df = train_df.sample(frac=1, random_state=14).groupby('label_group').nth(0)\n",
        "val_df['label_group'] = val_df.index\n",
        "val_df = val_df.set_index('index')\n",
        "train_df = train_df.drop('index', axis=1)\n",
        "train_df = train_df.drop(index=val_df.index).reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "A8qZubQ5YHRv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# shutil.rmtree(str(temp_dir), ignore_errors=True)"
      ],
      "metadata": {
        "id": "zkU51fPtt8h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(train_df['label_group'])"
      ],
      "metadata": {
        "id": "pK6tAinAn2YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "data_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "     layers.RandomRotation(0.5),\n",
        "     layers.RandomContrast(0.5),\n",
        "     layers.RandomZoom((-0.2, 0.2)),\n",
        "     layers.RandomTranslation(0.14, 0.14),\n",
        "     ]\n",
        ")\n",
        "\n",
        "def filepath_to_img(filepath):\n",
        "    img_string = tf.io.read_file(filepath)\n",
        "    img = tf.image.decode_jpeg(img_string)\n",
        "\n",
        "    return img\n",
        "\n",
        "def configure_dataset(ds, mode, fine_tune=False):\n",
        "    assert mode == 'training' or mode == 'validation', \"mode should be 'training' or 'validation'\" \n",
        "    \n",
        "    ds = ds.map(lambda filepath, label: (filepath_to_img(filepath), label), \n",
        "                num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    ds = ds.map(lambda img, label: (layers.Resizing(224, 224, crop_to_aspect_ratio=True)(img), label),\n",
        "                num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = ds.map(lambda img, label: (data_augmentation(img), label), \n",
        "                num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    if not fine_tune:               # we'll use some available preprocessing in keras model\n",
        "        ds = ds.map(lambda img, label: (layers.Rescaling(1./255, offset=-1)(img), label),\n",
        "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    ds = ds.prefetch(buffer_size=2048)\n",
        "    ds = ds.batch(batch_size)\n",
        "    \n",
        "    return ds"
      ],
      "metadata": {
        "id": "pAnPUmVTSG6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model and utilities"
      ],
      "metadata": {
        "id": "cQtlK-VCzbkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define embedding model (Run only once)\n",
        "\n",
        "def get_embedding_model(first_trainable_layer=200):\n",
        "    base_model = applications.MobileNetV3Large(weights='imagenet', input_shape=(224, 224, 3), include_top=False, include_preprocessing=True)\n",
        "\n",
        "    output = layers.Flatten()(base_model.output)\n",
        "    output = layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(output)\n",
        "\n",
        "    embedding = Model(base_model.input, output, name='Embedding')\n",
        "\n",
        "    trainable = False\n",
        "    for i in range(len(base_model.layers)):\n",
        "        if i == first_trainable_layer:\n",
        "            trainable = True\n",
        "        base_model.layers[i].trainable = trainable\n",
        "\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "IWdGJUGlXDqr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define custom callback\n",
        "\n",
        "def get_summary_writer(name):\n",
        "    log_dir = 'logs/' + name\n",
        "    return {\n",
        "        'train': tf.summary.create_file_writer(log_dir + '/train'),\n",
        "        'val': tf.summary.create_file_writer(log_dir + '/val'),\n",
        "        'lr': tf.summary.create_file_writer(log_dir + '/lr'),\n",
        "        'best_weights': None,\n",
        "        'epoch_loss' : [],\n",
        "        'epoch_val_loss': []\n",
        "\n",
        "    }\n",
        "\n",
        "image_summary_writer = tf.summary.create_file_writer('logs/images')\n",
        "\n",
        "class MyCallback(callbacks.Callback):\n",
        "    def __init__(self, writers, reduce_lr=True, restore_best_weights=True):\n",
        "        super(MyCallback, self).__init__()\n",
        "\n",
        "        self.reduce_lr = reduce_lr\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.writers = writers\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "\n",
        "        with self.writers['lr'].as_default():\n",
        "            tf.summary.scalar('learning rate', backend.get_value(self.model.optimizer.lr), step=len(self.writers['epoch_val_loss']))\n",
        "\n",
        "        if self.reduce_lr and len(self.writers['epoch_loss']) > 0 and logs.get('loss') > self.writers['epoch_loss'][-1]:\n",
        "            new_lr = backend.get_value(self.model.optimizer.lr) / 10.0\n",
        "            backend.set_value(self.model.optimizer.lr, new_lr)\n",
        "            print(\"\\n[MyCallback]: Learning rate reduced to {}.\".format(new_lr))\n",
        "\n",
        "        if self.restore_best_weights and (len(self.writers['epoch_val_loss']) == 0 or logs.get('val_loss') < min(self.writers['epoch_val_loss']) - 1e-4):\n",
        "            self.writers['best_weights'] = self.model.get_weights()\n",
        "            print(\"\\n[MyCallback]: Best weights on val_loss saved to 'best_weight'.\")\n",
        "\n",
        "        self.writers['epoch_loss'].append(logs.get('loss'))\n",
        "        self.writers['epoch_val_loss'].append(logs.get('val_loss'))\n",
        "\n",
        "        with self.writers['train'].as_default():\n",
        "            tf.summary.scalar('loss', logs.get('loss'), step=len(self.writers['epoch_loss']) - 1)\n",
        "        \n",
        "        with self.writers['val'].as_default():\n",
        "            tf.summary.scalar('loss', logs.get('val_loss'), step=len(self.writers['epoch_val_loss']) - 1)"
      ],
      "metadata": {
        "id": "hhKyzYjy0sk1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_ds(train_df):\n",
        "    train_df = train_df.sample(frac=1.0).reset_index(drop=True)\n",
        "    train_ds = pd.concat([train_df, train_df]).sort_index(axis=0)\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((str(train_img_dir) + os.sep + train_ds['image'], label_encoder.transform(train_ds['label_group'])))\n",
        "    train_ds = configure_dataset(train_ds, mode='training', fine_tune=True)     # training using keras pretrained-model\n",
        "\n",
        "    return train_ds"
      ],
      "metadata": {
        "id": "0FxDZSyYnuup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = pd.concat([val_df, val_df]).sort_index(axis=0)\n",
        "val_labels = label_encoder.transform(val_ds['label_group'])\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((str(train_img_dir) + os.sep + val_ds['image'], val_labels))\n",
        "val_ds = configure_dataset(val_ds, mode='validation', fine_tune=True)   # evaluation using keras pretrained-model"
      ],
      "metadata": {
        "id": "1X9BQITyClpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot some training images\n",
        "ncols = 3\n",
        "nrows = 3\n",
        "plt.figure(figsize=(4*ncols, 4*nrows))\n",
        "spec = gridspec.GridSpec(ncols=ncols, nrows=nrows)\n",
        "for img_batch, label_batch in get_train_ds(train_df).take(1):\n",
        "    with image_summary_writer.as_default():\n",
        "            tf.summary.image('training image', img_batch/255, max_outputs=64, step=0)\n",
        "    i = 0\n",
        "    for img, label in zip(img_batch, label_batch):\n",
        "        plt.subplot(spec[i])\n",
        "        plt.imshow(img/255)\n",
        "        plt.title(label.numpy())\n",
        "        plt.axis('off')\n",
        "\n",
        "        i += 1\n",
        "        if i == nrows * ncols:\n",
        "            break\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cetzlq5IUJ0G",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(embedding, num_epoch, writers):\n",
        "\n",
        "    max_epoch = num_epoch + len(writers['epoch_loss'])\n",
        "    for _ in range(num_epoch):\n",
        "        train_ds = get_train_ds(train_df)\n",
        "\n",
        "        print('Epoch %d/%d' % (len(writers['epoch_loss']) + 1, max_epoch))\n",
        "\n",
        "        embedding.fit(\n",
        "            train_ds,\n",
        "            epochs=1,\n",
        "            validation_data=val_ds,\n",
        "            callbacks=[MyCallback(writers, reduce_lr=True, restore_best_weights=True)]\n",
        "        )"
      ],
      "metadata": {
        "id": "hwstKXvA74X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate model"
      ],
      "metadata": {
        "id": "1Dh1nO43z3I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semihard_embedding = get_embedding_model()\n",
        "semihard_summary_writers = get_summary_writer('semihard')\n",
        "\n",
        "semihard_embedding.compile(\n",
        "    optimizer=SGD(learning_rate=8e-3, momentum=0.9, nesterov=True),\n",
        "    loss=tfa.losses.TripletSemiHardLoss(),\n",
        "    )\n",
        "\n",
        "train(semihard_embedding, 8, semihard_summary_writers)"
      ],
      "metadata": {
        "id": "j7GEawF90rqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = semihard_embedding"
      ],
      "metadata": {
        "id": "CSJmBrw7T74L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create anchor embeddings\n",
        "\n",
        "%%time\n",
        "\n",
        "set_seed(42)\n",
        "anchor_df = train_df.groupby('label_group').nth(0)\n",
        "\n",
        "anchor_df['label_group'] = anchor_df.index\n",
        "anchor_df = anchor_df.reset_index(drop=True)\n",
        "\n",
        "num_per_anchor = 4\n",
        "\n",
        "anchor_df = pd.concat([anchor_df for _ in range(num_per_anchor)]).sort_index(axis=0)\n",
        "anchor_labels = label_encoder.transform(anchor_df['label_group'])\n",
        "anchor_ds = tf.data.Dataset.from_tensor_slices((str(train_img_dir) + os.sep + anchor_df['image'], anchor_labels))\n",
        "anchor_ds = configure_dataset(anchor_ds, mode='training', fine_tune=True)\n",
        "anchor_embeddings = embedding.predict(anchor_ds)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "decomposed_embeddings = pca.fit_transform(anchor_embeddings)\n",
        "\n",
        "print('Anchor embeddings created')"
      ],
      "metadata": {
        "id": "RMaBhhzUvzcA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Use KNN for classification task\n",
        "\n",
        "%%time\n",
        "KNN = KNeighborsClassifier(n_neighbors=num_per_anchor)\n",
        "KNN.fit(anchor_embeddings, anchor_labels)"
      ],
      "metadata": {
        "id": "CnhuWTCbvFCW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate embeddings for validation set\n",
        "\n",
        "%%time\n",
        "\n",
        "val_embeddings = embedding.predict(val_ds)"
      ],
      "metadata": {
        "id": "Z5JU3lRP6-7S",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/checkpoint/anchor_embeddings.pkl', 'wb') as f:\n",
        "    pickle.dump(anchor_embeddings, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/checkpoint/anchor_labels.pkl', 'wb') as f:\n",
        "    pickle.dump(anchor_labels, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/checkpoint/val_embeddings.pkl', 'wb') as f:\n",
        "    pickle.dump(val_embeddings, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/checkpoint/val_labels.pkl', 'wb') as f:\n",
        "    pickle.dump(val_labels, f)"
      ],
      "metadata": {
        "id": "g9Pc8NUmKZC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Predict class probabilities for each validation image\n",
        "\n",
        "%%time\n",
        "\n",
        "val_preds = KNN.predict_proba(val_embeddings)"
      ],
      "metadata": {
        "id": "RlJXJ2Z97sis",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Top-K accuracy\n",
        "\n",
        "k =  1#@param {type: \"integer\"}\n",
        "\n",
        "top_k_accuracy_score(val_labels, val_preds, k=k)"
      ],
      "metadata": {
        "id": "61IrN1Af8P42",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize anchor embeddings on 2D plane\n",
        "\n",
        "num_class = 30 #@param {type: 'integer'}\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.scatterplot(x=decomposed_embeddings[:num_per_anchor * num_class, 0], \n",
        "                y=decomposed_embeddings[:num_per_anchor * num_class, 1], \n",
        "                hue=anchor_labels[:num_per_anchor * num_class], \n",
        "                palette=cm.get_cmap('Spectral')\n",
        ")"
      ],
      "metadata": {
        "id": "5H9Ka-6UERG5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semihard_embedding.save('/content/drive/MyDrive/checkpoint/semihard_embedding')"
      ],
      "metadata": {
        "id": "d3OCL7R_TvO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semihard_embedding = tf.keras.models.load_model('/content/drive/MyDrive/checkpoint/semihard_embedding')"
      ],
      "metadata": {
        "id": "J7Lf-h0GW8Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epoch = len(epoch_loss_histories['loss'])\n",
        "plt.plot(range(max_epoch), epoch_loss_histories['loss'], label='loss')\n",
        "plt.plot(range(max_epoch), epoch_loss_histories['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "723h-3AsOnZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "m_sNem7C-fbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/checkpoint/logs"
      ],
      "metadata": {
        "id": "yCQt_pmtl-2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "3k201Hi03KoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard dev upload \\\n",
        "  --logdir logs/fit \\\n",
        "  --name \"(optional) My latest experiment\" \\\n",
        "  --description \"(optional) Simple comparison of several hyperparameters\" \\\n",
        "  --one_shot"
      ],
      "metadata": {
        "id": "xmBm23jOBp1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorboard.plugins import projector\n",
        "\n",
        "\n",
        "\n",
        "# Set up a logs directory, so Tensorboard knows where to look for files.\n",
        "log_dir='/logs/imdb-example/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# Save Labels separately on a line-by-line manner.\n",
        "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
        "  for subwords in encoder.subwords:\n",
        "    f.write(\"{}\\n\".format(subwords))\n",
        "  # Fill in the rest of the labels with \"unknown\".\n",
        "  for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):\n",
        "    f.write(\"unknown #{}\\n\".format(unknown))\n",
        "\n",
        "\n",
        "# Save the weights we want to analyze as a variable. Note that the first\n",
        "# value represents any unknown word, which is not in the metadata, here\n",
        "# we will remove this value.\n",
        "weights = tf.Variable(model.layers[0].get_weights()[0][1:])\n",
        "# Create a checkpoint from embedding, the filename and key are the\n",
        "# name of the tensor.\n",
        "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
        "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
        "\n",
        "# Set up config.\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
        "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "embedding.metadata_path = 'metadata.tsv'\n",
        "projector.visualize_embeddings(log_dir, config)"
      ],
      "metadata": {
        "id": "-mnZ5sLPNfLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras"
      ],
      "metadata": {
        "id": "mDP0HbZQQwFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Implements triplet loss.\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow_addons.losses import metric_learning\n",
        "from tensorflow_addons.utils.keras_utils import LossFunctionWrapper\n",
        "from tensorflow_addons.utils.types import FloatTensorLike, TensorLike\n",
        "from typeguard import typechecked\n",
        "from typing import Optional, Union, Callable\n",
        "\n",
        "\n",
        "def _masked_maximum(data, mask, dim=1):\n",
        "    \"\"\"Computes the axis wise maximum over chosen elements.\n",
        "\n",
        "    Args:\n",
        "      data: 2-D float `Tensor` of shape `[n, m]`.\n",
        "      mask: 2-D Boolean `Tensor` of shape `[n, m]`.\n",
        "      dim: The dimension over which to compute the maximum.\n",
        "\n",
        "    Returns:\n",
        "      masked_maximums: N-D `Tensor`.\n",
        "        The maximized dimension is of size 1 after the operation.\n",
        "    \"\"\"\n",
        "    axis_minimums = tf.math.reduce_min(data, dim, keepdims=True)\n",
        "    masked_maximums = (\n",
        "        tf.math.reduce_max(\n",
        "            tf.math.multiply(data - axis_minimums, mask), dim, keepdims=True\n",
        "        )\n",
        "        + axis_minimums\n",
        "    )\n",
        "    return masked_maximums\n",
        "\n",
        "\n",
        "def _masked_minimum(data, mask, dim=1):\n",
        "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
        "\n",
        "    Args:\n",
        "      data: 2-D float `Tensor` of shape `[n, m]`.\n",
        "      mask: 2-D Boolean `Tensor` of shape `[n, m]`.\n",
        "      dim: The dimension over which to compute the minimum.\n",
        "\n",
        "    Returns:\n",
        "      masked_minimums: N-D `Tensor`.\n",
        "        The minimized dimension is of size 1 after the operation.\n",
        "    \"\"\"\n",
        "    axis_maximums = tf.math.reduce_max(data, dim, keepdims=True)\n",
        "    masked_minimums = (\n",
        "        tf.math.reduce_min(\n",
        "            tf.math.multiply(data - axis_maximums, mask), dim, keepdims=True\n",
        "        )\n",
        "        + axis_maximums\n",
        "    )\n",
        "    return masked_minimums\n",
        "\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
        "@tf.function\n",
        "def triplet_semihard_loss(\n",
        "    y_true: TensorLike,\n",
        "    y_pred: TensorLike,\n",
        "    margin: FloatTensorLike = 1.0,\n",
        "    distance_metric: Union[str, Callable] = \"L2\",\n",
        ") -> tf.Tensor:\n",
        "    r\"\"\"Computes the triplet loss with semi-hard negative mining.\n",
        "\n",
        "    Usage:\n",
        "\n",
        "    >>> y_true = tf.convert_to_tensor([0, 0])\n",
        "    >>> y_pred = tf.convert_to_tensor([[0.0, 1.0], [1.0, 0.0]])\n",
        "    >>> tfa.losses.triplet_semihard_loss(y_true, y_pred, distance_metric=\"L2\")\n",
        "    <tf.Tensor: shape=(), dtype=float32, numpy=2.4142137>\n",
        "\n",
        "    >>> # Calling with callable `distance_metric`\n",
        "    >>> distance_metric = lambda x: tf.linalg.matmul(x, x, transpose_b=True)\n",
        "    >>> tfa.losses.triplet_semihard_loss(y_true, y_pred, distance_metric=distance_metric)\n",
        "    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>\n",
        "\n",
        "    Args:\n",
        "      y_true: 1-D integer `Tensor` with shape `[batch_size]` of\n",
        "        multiclass integer labels.\n",
        "      y_pred: 2-D float `Tensor` of embedding vectors. Embeddings should\n",
        "        be l2 normalized.\n",
        "      margin: Float, margin term in the loss definition.\n",
        "      distance_metric: `str` or a `Callable` that determines distance metric.\n",
        "        Valid strings are \"L2\" for l2-norm distance,\n",
        "        \"squared-L2\" for squared l2-norm distance,\n",
        "        and \"angular\" for cosine similarity.\n",
        "\n",
        "        A `Callable` should take a batch of embeddings as input and\n",
        "        return the pairwise distance matrix.\n",
        "\n",
        "    Returns:\n",
        "      triplet_loss: float scalar with dtype of `y_pred`.\n",
        "    \"\"\"\n",
        "    labels = tf.convert_to_tensor(y_true, name=\"labels\")\n",
        "    embeddings = tf.convert_to_tensor(y_pred, name=\"embeddings\")\n",
        "\n",
        "    convert_to_float32 = (\n",
        "        embeddings.dtype == tf.dtypes.float16 or embeddings.dtype == tf.dtypes.bfloat16\n",
        "    )\n",
        "    precise_embeddings = (\n",
        "        tf.cast(embeddings, tf.dtypes.float32) if convert_to_float32 else embeddings\n",
        "    )\n",
        "\n",
        "    # Reshape label tensor to [batch_size, 1].\n",
        "    lshape = tf.shape(labels)\n",
        "    labels = tf.reshape(labels, [lshape[0], 1])\n",
        "\n",
        "    # Build pairwise squared distance matrix\n",
        "\n",
        "    if distance_metric == \"L2\":\n",
        "        pdist_matrix = metric_learning.pairwise_distance(\n",
        "            precise_embeddings, squared=False\n",
        "        )\n",
        "\n",
        "    elif distance_metric == \"squared-L2\":\n",
        "        pdist_matrix = metric_learning.pairwise_distance(\n",
        "            precise_embeddings, squared=True\n",
        "        )\n",
        "\n",
        "    elif distance_metric == \"angular\":\n",
        "        pdist_matrix = metric_learning.angular_distance(precise_embeddings)\n",
        "\n",
        "    else:\n",
        "        pdist_matrix = distance_metric(precise_embeddings)\n",
        "\n",
        "    # Build pairwise binary adjacency matrix.\n",
        "    adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
        "    # Invert so we can select negatives only.\n",
        "    adjacency_not = tf.math.logical_not(adjacency)\n",
        "\n",
        "    batch_size = tf.size(labels)\n",
        "\n",
        "    # Compute the mask.\n",
        "    pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n",
        "    mask = tf.math.logical_and(\n",
        "        tf.tile(adjacency_not, [batch_size, 1]),\n",
        "        tf.math.greater(\n",
        "            pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])\n",
        "        ),\n",
        "    )\n",
        "    mask_final = tf.reshape(\n",
        "        tf.math.greater(\n",
        "            tf.math.reduce_sum(\n",
        "                tf.cast(mask, dtype=tf.dtypes.float32), 1, keepdims=True\n",
        "            ),\n",
        "            0.0,\n",
        "        ),\n",
        "        [batch_size, batch_size],\n",
        "    )\n",
        "    mask_final = tf.transpose(mask_final)\n",
        "\n",
        "    adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.dtypes.float32)\n",
        "\n",
        "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
        "    negatives_outside = tf.reshape(\n",
        "        _masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size]\n",
        "    )\n",
        "    negatives_outside = tf.transpose(negatives_outside)\n",
        "\n",
        "    # negatives_inside: largest D_an.\n",
        "    negatives_inside = tf.tile(\n",
        "        _masked_maximum(pdist_matrix, adjacency_not), [1, batch_size]\n",
        "    )\n",
        "    semi_hard_negatives = tf.where(mask_final, negatives_outside, negatives_inside)\n",
        "\n",
        "    loss_mat = tf.math.add(margin, pdist_matrix - semi_hard_negatives)\n",
        "\n",
        "    mask_positives = tf.cast(adjacency, dtype=tf.dtypes.float32) - tf.linalg.diag(\n",
        "        tf.ones([batch_size])\n",
        "    )\n",
        "\n",
        "    # In lifted-struct, the authors multiply 0.5 for upper triangular\n",
        "    #   in semihard, they take all positive pairs except the diagonal.\n",
        "    num_positives = tf.math.reduce_sum(mask_positives)\n",
        "\n",
        "    triplet_loss = tf.math.truediv(\n",
        "        tf.math.reduce_sum(\n",
        "            tf.math.maximum(tf.math.multiply(loss_mat, mask_positives), 0.0)\n",
        "        ),\n",
        "        num_positives,\n",
        "    )\n",
        "\n",
        "    if convert_to_float32:\n",
        "        return tf.cast(triplet_loss, embeddings.dtype)\n",
        "    else:\n",
        "        return triplet_loss\n",
        "\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
        "@tf.function\n",
        "def triplet_hard_loss(\n",
        "    y_true: TensorLike,\n",
        "    y_pred: TensorLike,\n",
        "    margin: FloatTensorLike = 1.0,\n",
        "    soft: bool = False,\n",
        "    distance_metric: Union[str, Callable] = \"L2\",\n",
        ") -> tf.Tensor:\n",
        "    r\"\"\"Computes the triplet loss with hard negative and hard positive mining.\n",
        "\n",
        "    Usage:\n",
        "\n",
        "    >>> y_true = tf.convert_to_tensor([0, 0])\n",
        "    >>> y_pred = tf.convert_to_tensor([[0.0, 1.0], [1.0, 0.0]])\n",
        "    >>> tfa.losses.triplet_hard_loss(y_true, y_pred, distance_metric=\"L2\")\n",
        "    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>\n",
        "\n",
        "    >>> # Calling with callable `distance_metric`\n",
        "    >>> distance_metric = lambda x: tf.linalg.matmul(x, x, transpose_b=True)\n",
        "    >>> tfa.losses.triplet_hard_loss(y_true, y_pred, distance_metric=distance_metric)\n",
        "    <tf.Tensor: shape=(), dtype=float32, numpy=0.0>\n",
        "\n",
        "    Args:\n",
        "      y_true: 1-D integer `Tensor` with shape `[batch_size]` of\n",
        "        multiclass integer labels.\n",
        "      y_pred: 2-D float `Tensor` of embedding vectors. Embeddings should\n",
        "        be l2 normalized.\n",
        "      margin: Float, margin term in the loss definition.\n",
        "      soft: Boolean, if set, use the soft margin version.\n",
        "      distance_metric: `str` or a `Callable` that determines distance metric.\n",
        "        Valid strings are \"L2\" for l2-norm distance,\n",
        "        \"squared-L2\" for squared l2-norm distance,\n",
        "        and \"angular\" for cosine similarity.\n",
        "\n",
        "        A `Callable` should take a batch of embeddings as input and\n",
        "        return the pairwise distance matrix.\n",
        "\n",
        "    Returns:\n",
        "      triplet_loss: float scalar with dtype of `y_pred`.\n",
        "    \"\"\"\n",
        "    labels = tf.convert_to_tensor(y_true, name=\"labels\")\n",
        "    embeddings = tf.convert_to_tensor(y_pred, name=\"embeddings\")\n",
        "\n",
        "    convert_to_float32 = (\n",
        "        embeddings.dtype == tf.dtypes.float16 or embeddings.dtype == tf.dtypes.bfloat16\n",
        "    )\n",
        "    precise_embeddings = (\n",
        "        tf.cast(embeddings, tf.dtypes.float32) if convert_to_float32 else embeddings\n",
        "    )\n",
        "\n",
        "    # Reshape label tensor to [batch_size, 1].\n",
        "    lshape = tf.shape(labels)\n",
        "    labels = tf.reshape(labels, [lshape[0], 1])\n",
        "\n",
        "    # Build pairwise squared distance matrix.\n",
        "    if distance_metric == \"L2\":\n",
        "        pdist_matrix = metric_learning.pairwise_distance(\n",
        "            precise_embeddings, squared=False\n",
        "        )\n",
        "\n",
        "    elif distance_metric == \"squared-L2\":\n",
        "        pdist_matrix = metric_learning.pairwise_distance(\n",
        "            precise_embeddings, squared=True\n",
        "        )\n",
        "\n",
        "    elif distance_metric == \"angular\":\n",
        "        pdist_matrix = metric_learning.angular_distance(precise_embeddings)\n",
        "\n",
        "    else:\n",
        "        pdist_matrix = distance_metric(precise_embeddings)\n",
        "\n",
        "    # Build pairwise binary adjacency matrix.\n",
        "    adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
        "    # Invert so we can select negatives only.\n",
        "    adjacency_not = tf.math.logical_not(adjacency)\n",
        "\n",
        "    adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n",
        "    # hard negatives: smallest D_an.\n",
        "    hard_negatives = _masked_minimum(pdist_matrix, adjacency_not)\n",
        "\n",
        "    batch_size = tf.size(labels)\n",
        "\n",
        "    adjacency = tf.cast(adjacency, dtype=tf.dtypes.float32)\n",
        "\n",
        "    mask_positives = tf.cast(adjacency, dtype=tf.dtypes.float32) - tf.linalg.diag(\n",
        "        tf.ones([batch_size])\n",
        "    )\n",
        "\n",
        "    # hard positives: largest D_ap.\n",
        "    hard_positives = _masked_maximum(pdist_matrix, mask_positives)\n",
        "\n",
        "    if soft:\n",
        "        triplet_loss = tf.math.log1p(tf.math.exp(hard_positives - hard_negatives))\n",
        "    else:\n",
        "        triplet_loss = tf.maximum(hard_positives - hard_negatives + margin, 0.0)\n",
        "\n",
        "    # Get final mean triplet loss\n",
        "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
        "\n",
        "    if convert_to_float32:\n",
        "        return tf.cast(triplet_loss, embeddings.dtype)\n",
        "    else:\n",
        "        return triplet_loss\n",
        "\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
        "class TripletSemiHardLoss(LossFunctionWrapper):\n",
        "    \"\"\"Computes the triplet loss with semi-hard negative mining.\n",
        "\n",
        "    The loss encourages the positive distances (between a pair of embeddings\n",
        "    with the same labels) to be smaller than the minimum negative distance\n",
        "    among which are at least greater than the positive distance plus the\n",
        "    margin constant (called semi-hard negative) in the mini-batch.\n",
        "    If no such negative exists, uses the largest negative distance instead.\n",
        "    See: https://arxiv.org/abs/1503.03832.\n",
        "\n",
        "    We expect labels `y_true` to be provided as 1-D integer `Tensor` with shape\n",
        "    `[batch_size]` of multi-class integer labels. And embeddings `y_pred` must be\n",
        "    2-D float `Tensor` of l2 normalized embedding vectors.\n",
        "\n",
        "    Args:\n",
        "      margin: Float, margin term in the loss definition. Default value is 1.0.\n",
        "      name: Optional name for the op.\n",
        "    \"\"\"\n",
        "\n",
        "    @typechecked\n",
        "    def __init__(\n",
        "        self,\n",
        "        margin: FloatTensorLike = 1.0,\n",
        "        distance_metric: Union[str, Callable] = \"L2\",\n",
        "        name: Optional[str] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            triplet_semihard_loss,\n",
        "            name=name,\n",
        "            reduction=tf.keras.losses.Reduction.NONE,\n",
        "            margin=margin,\n",
        "            distance_metric=distance_metric,\n",
        "        )\n",
        "\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
        "class TripletHardLoss(LossFunctionWrapper):\n",
        "    \"\"\"Computes the triplet loss with hard negative and hard positive mining.\n",
        "\n",
        "    The loss encourages the maximum positive distance (between a pair of embeddings\n",
        "    with the same labels) to be smaller than the minimum negative distance plus the\n",
        "    margin constant in the mini-batch.\n",
        "    The loss selects the hardest positive and the hardest negative samples\n",
        "    within the batch when forming the triplets for computing the loss.\n",
        "    See: https://arxiv.org/pdf/1703.07737.\n",
        "\n",
        "    We expect labels `y_true` to be provided as 1-D integer `Tensor` with shape\n",
        "    `[batch_size]` of multi-class integer labels. And embeddings `y_pred` must be\n",
        "    2-D float `Tensor` of l2 normalized embedding vectors.\n",
        "\n",
        "    Args:\n",
        "      margin: Float, margin term in the loss definition. Default value is 1.0.\n",
        "      soft: Boolean, if set, use the soft margin version. Default value is False.\n",
        "      name: Optional name for the op.\n",
        "    \"\"\"\n",
        "\n",
        "    @typechecked\n",
        "    def __init__(\n",
        "        self,\n",
        "        margin: FloatTensorLike = 1.0,\n",
        "        soft: bool = False,\n",
        "        distance_metric: Union[str, Callable] = \"L2\",\n",
        "        name: Optional[str] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            triplet_hard_loss,\n",
        "            name=name,\n",
        "            reduction=tf.keras.losses.Reduction.NONE,\n",
        "            margin=margin,\n",
        "            soft=soft,\n",
        "            distance_metric=distance_metric,\n",
        "        )"
      ],
      "metadata": {
        "id": "WQbLkAuAPI7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hard có soft"
      ],
      "metadata": {
        "id": "fv6YhTn7bY8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def triplet_hard_loss(\n",
        "    y_true: TensorLike,\n",
        "    y_pred: TensorLike,\n",
        "    margin: FloatTensorLike = 1.0,\n",
        "    ##############################\n",
        "    soft: bool = False,          #\n",
        "    ##############################\n",
        "    distance_metric: Union[str, Callable] = \"L2\",\n",
        ") -> tf.Tensor:\n",
        "    r\"\"\"Computes the triplet loss with hard negative and hard positive mining.\n",
        "\n",
        "    Usage:\n",
        "\n",
        "    >>> y_true = tf.convert_to_tensor([0, 0])\n",
        "    >>> y_pred = tf.convert_to_tensor([[0.0, 1.0], [1.0, 0.0]])\n",
        "    >>> tfa.losses.triplet_hard_loss(y_true, y_pred, distance_metric=\"L2\")\n",
        "    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>\n",
        "\n",
        "    >>> # Calling with callable `distance_metric`\n",
        "    >>> distance_metric = lambda x: tf.linalg.matmul(x, x, transpose_b=True)\n",
        "    >>> tfa.losses.triplet_hard_loss(y_true, y_pred, distance_metric=distance_metric)\n",
        "    <tf.Tensor: shape=(), dtype=float32, numpy=0.0>\n",
        "\n",
        "    Args:\n",
        "      y_true: 1-D integer `Tensor` with shape `[batch_size]` of\n",
        "        multiclass integer labels.\n",
        "      y_pred: 2-D float `Tensor` of embedding vectors. Embeddings should\n",
        "        be l2 normalized.\n",
        "      margin: Float, margin term in the loss definition.\n",
        "      soft: Boolean, if set, use the soft margin version.\n",
        "      distance_metric: `str` or a `Callable` that determines distance metric.\n",
        "        Valid strings are \"L2\" for l2-norm distance,\n",
        "        \"squared-L2\" for squared l2-norm distance,\n",
        "        and \"angular\" for cosine similarity.\n",
        "\n",
        "        A `Callable` should take a batch of embeddings as input and\n",
        "        return the pairwise distance matrix.\n",
        "\n",
        "    Returns:\n",
        "      triplet_loss: float scalar with dtype of `y_pred`.\n",
        "    \"\"\"\n",
        "    labels = tf.convert_to_tensor(y_true, name=\"labels\")\n",
        "    embeddings = tf.convert_to_tensor(y_pred, name=\"embeddings\")\n",
        "\n",
        "    convert_to_float32 = (\n",
        "        embeddings.dtype == tf.dtypes.float16 or embeddings.dtype == tf.dtypes.bfloat16\n",
        "    )\n",
        "    precise_embeddings = (\n",
        "        tf.cast(embeddings, tf.dtypes.float32) if convert_to_float32 else embeddings\n",
        "    )\n",
        "\n",
        "    # Reshape label tensor to [batch_size, 1].\n",
        "    lshape = tf.shape(labels)\n",
        "    labels = tf.reshape(labels, [lshape[0], 1])\n",
        "\n",
        "    # Build pairwise squared distance matrix.\n",
        "    if distance_metric == \"L2\":\n",
        "        pdist_matrix = metric_learning.pairwise_distance(\n",
        "            precise_embeddings, squared=False\n",
        "        )\n",
        "\n",
        "    elif distance_metric == \"squared-L2\":\n",
        "        pdist_matrix = metric_learning.pairwise_distance(\n",
        "            precise_embeddings, squared=True\n",
        "        )\n",
        "\n",
        "    elif distance_metric == \"angular\":\n",
        "        pdist_matrix = metric_learning.angular_distance(precise_embeddings)\n",
        "\n",
        "    else:\n",
        "        pdist_matrix = distance_metric(precise_embeddings)\n",
        "\n",
        "    # Build pairwise binary adjacency matrix.\n",
        "    adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
        "    # Invert so we can select negatives only.\n",
        "    adjacency_not = tf.math.logical_not(adjacency)\n",
        "\n",
        "    ###################################################################\n",
        "    adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)   #\n",
        "    # hard negatives: smallest D_an.                                  #\n",
        "    hard_negatives = _masked_minimum(pdist_matrix, adjacency_not)     #\n",
        "    ###################################################################\n",
        "\n",
        "\n",
        "    batch_size = tf.size(labels)\n",
        "    \n",
        "    #-------------------------------------------------------------------------------------------------\n",
        "    # Compute the mask.\n",
        "    pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])\n",
        "    mask = tf.math.logical_and(\n",
        "        tf.tile(adjacency_not, [batch_size, 1]),\n",
        "        tf.math.greater(\n",
        "            pdist_matrix_tile, tf.reshape(tf.transpose(pdist_matrix), [-1, 1])\n",
        "        ),\n",
        "    )\n",
        "    mask_final = tf.reshape(\n",
        "        tf.math.greater(\n",
        "            tf.math.reduce_sum(\n",
        "                tf.cast(mask, dtype=tf.dtypes.float32), 1, keepdims=True\n",
        "            ),\n",
        "            0.0,\n",
        "        ),\n",
        "        [batch_size, batch_size],\n",
        "    )\n",
        "    mask_final = tf.transpose(mask_final)\n",
        "\n",
        "    adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.dtypes.float32)\n",
        "\n",
        "    # negatives_outside: smallest D_an where D_an > D_ap.\n",
        "    negatives_outside = tf.reshape(\n",
        "        _masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size]\n",
        "    )\n",
        "    negatives_outside = tf.transpose(negatives_outside)\n",
        "\n",
        "    # negatives_inside: largest D_an.\n",
        "    negatives_inside = tf.tile(\n",
        "        _masked_maximum(pdist_matrix, adjacency_not), [1, batch_size]\n",
        "    )\n",
        "    semi_hard_negatives = tf.where(mask_final, negatives_outside, negatives_inside)\n",
        "\n",
        "    loss_mat = tf.math.add(margin, pdist_matrix - semi_hard_negatives)\n",
        "    #-------------------------------------------------------------------------------------------------\n",
        "    \n",
        "    \n",
        "    #############################################################\n",
        "    adjacency = tf.cast(adjacency, dtype=tf.dtypes.float32)     #\n",
        "    #############################################################\n",
        "\n",
        "    mask_positives = tf.cast(adjacency, dtype=tf.dtypes.float32) - tf.linalg.diag(\n",
        "        tf.ones([batch_size])\n",
        "    )\n",
        "\n",
        "    ##################################################################################################\n",
        "    # hard positives: largest D_ap.                                                                  #\n",
        "    hard_positives = _masked_maximum(pdist_matrix, mask_positives)                                   #\n",
        "                                                                                                     #\n",
        "    if soft:                                                                                         #\n",
        "        triplet_loss = tf.math.log1p(tf.math.exp(hard_positives - hard_negatives))                   #\n",
        "    else:                                                                                            #\n",
        "        triplet_loss = tf.maximum(hard_positives - hard_negatives + margin, 0.0)                     #\n",
        "                                                                                                     #\n",
        "    # Get final mean triplet loss                                                                    #\n",
        "    triplet_loss = tf.reduce_mean(triplet_loss)                                                      #\n",
        "    ##################################################################################################\n",
        "    \n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "    # In lifted-struct, the authors multiply 0.5 for upper triangular                               #-\n",
        "    #   in semihard, they take all positive pairs except the diagonal.\n",
        "    num_positives = tf.math.reduce_sum(mask_positives)\n",
        "\n",
        "    triplet_loss = tf.math.truediv(\n",
        "        tf.math.reduce_sum(\n",
        "            tf.math.maximum(tf.math.multiply(loss_mat, mask_positives), 0.0)\n",
        "        ),\n",
        "        num_positives,\n",
        "    )                                                                                               #-\n",
        "    # ------------------------------------------------------------------------------------------------\n",
        "\n",
        "    if convert_to_float32:\n",
        "        return tf.cast(triplet_loss, embeddings.dtype)\n",
        "    else:\n",
        "        return triplet_loss"
      ],
      "metadata": {
        "id": "PSDi3uzjUmMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "QQjCtW_uW8HK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}