{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamese_net_tf.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/calibretaliation/DeepLearning20211/blob/main/siamese_net_tf.ipynb",
      "authorship_tag": "ABX9TyORm7SMMAtkSCpuAfSOCnm3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calibretaliation/DeepLearning20211/blob/main/siamese_net_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "istTBFbsNPcR",
        "outputId": "203e68e0-39a7-4d54-ac1f-286c3a539e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepLearning20211'...\n",
            "remote: Enumerating objects: 32529, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 32529 (delta 37), reused 0 (delta 0), pack-reused 32466\u001b[K\n",
            "Receiving objects: 100% (32529/32529), 1.68 GiB | 25.13 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n",
            "Checking out files: 100% (32428/32428), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/calibretaliation/DeepLearning20211.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "CeDeB7H-A5Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "id": "cUOIsVncQGib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f50891-65ee-424f-d5e1-90160370036f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/tensorflow/docs\n",
            "  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-bv6pnipg\n",
            "  Running command git clone -q https://github.com/tensorflow/docs /tmp/pip-req-build-bv6pnipg\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (0.8.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (0.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (2.11.3)\n",
            "Requirement already satisfied: protobuf>=3.14 in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (3.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from tensorflow-docs==0.0.0.dev0) (3.13)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.14->tensorflow-docs==0.0.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->tensorflow-docs==0.0.0.dev0) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "import math\n",
        "import random\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from tensorflow.keras import applications, layers, losses, optimizers, metrics, Model, callbacks\n",
        "from tensorflow.keras.applications import resnet, mobilenet_v3\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.modeling\n",
        "import tensorflow_docs.plots\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "metadata": {
        "id": "WJyjcOIXQTWO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load CSV\n",
        "\n",
        "train_df = pd.read_csv('DeepLearning20211/data/train.csv')\n",
        "test_df = pd.read_csv('DeepLearning20211/data/test.csv')\n",
        "\n",
        "train_img_dir = Path('DeepLearning20211/data/train_images')\n",
        "test_img_dir = Path('DeepLearning20211/data/test_images/')"
      ],
      "metadata": {
        "id": "0NZlAgwOQj9q",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Drop images with duplicated p-hash value\n",
        "\n",
        "train_df = train_df.drop_duplicates(subset=['image_phash']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "3t5aEmV_RDm-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot class cardinality distribution\n",
        "\n",
        "# calculate number of images in each class\n",
        "label_count = train_df['label_group'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "label_count.value_counts().plot.bar()\n",
        "plt.xlabel('Class cardinality')\n",
        "plt.ylabel('Number of class')\n",
        "plt.title('Shopee Class cardinality distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w8c7i4HsoEeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create validation set\n",
        "val_df = train_df.sample(frac=1, random_state=14).groupby('label_group').nth(0)\n",
        "val_df['label_group'] = val_df.index\n",
        "val_df = val_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "A8qZubQ5YHRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dataset"
      ],
      "metadata": {
        "id": "G2uFtS8S6kM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Take one instance in each class as anchor\n",
        "\n",
        "set_seed(42)\n",
        "anchor_df = train_df.groupby('label_group').nth(0)"
      ],
      "metadata": {
        "id": "RMaBhhzUvzcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_triplets = 200"
      ],
      "metadata": {
        "id": "ildDalo7omg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(mode):\n",
        "    assert mode == 'training' or mode == 'validation', \"argument mode should be 'training' or 'validation'\" \n",
        "    df = train_df if mode == 'training' else val_df\n",
        "\n",
        "    # container to store image path\n",
        "    anchor_container = []\n",
        "    positive_container = []\n",
        "    negative_container = []\n",
        "\n",
        "    # for each anchor image, create num_triplets triplets\n",
        "    for anchor_label in anchor_df.index:\n",
        "        anchor = anchor_df['image'][anchor_label]\n",
        "\n",
        "        positive_df = df[df['label_group'] == anchor_label]\n",
        "        if len(positive_df) > 1:\n",
        "            positive_df = positive_df[positive_df['image'] != anchor]\n",
        "\n",
        "        negative_df = df.groupby('label_group').nth(0)\n",
        "        negative_df['label_group'] = negative_df.index\n",
        "        negative_df = negative_df.reset_index(drop=True)\n",
        "        negative_df = negative_df[negative_df['label_group'] != anchor_label]\n",
        "        for _ in range(num_triplets if mode == 'training' else 3):\n",
        "            anchor_container.append(str(train_img_dir) + os.sep + anchor)\n",
        "            positive_container.append(str(train_img_dir) + os.sep + np.squeeze(positive_df.sample(1)['image']))\n",
        "            negative_container.append(str(train_img_dir) + os.sep + np.squeeze(negative_df.sample(1)['image']))\n",
        "\n",
        "    return anchor_container, positive_container, negative_container"
      ],
      "metadata": {
        "id": "iT0Gb-nbIjA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_ds_raw = create_dataset(mode='training')\n",
        "# val_ds_raw = create_dataset(mode='validation')"
      ],
      "metadata": {
        "id": "wAYk5KOjS6VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('DeepLearning20211/train_ds_raw.pkl', 'wb') as f:\n",
        "#     pickle.dump(train_ds_raw, f)\n",
        "\n",
        "# with open('DeepLearning20211/val_ds_raw.pkl', 'wb') as f:\n",
        "#     pickle.dump(val_ds_raw, f)"
      ],
      "metadata": {
        "id": "afwwOS41RA-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('DeepLearning20211/train_ds_raw.pkl', 'rb') as f:\n",
        "#     train_ds_raw = pickle.load(f)\n",
        "\n",
        "# with open('DeepLearning20211/val_ds_raw.pkl', 'rb') as f:\n",
        "#     val_ds_raw = pickle.load(f)"
      ],
      "metadata": {
        "id": "H4cuUZ4TV9Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/train_ds_raw.pkl', 'rb') as f:\n",
        "    train_ds_raw = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/val_ds_raw.pkl', 'rb') as f:\n",
        "    val_ds_raw = pickle.load(f)"
      ],
      "metadata": {
        "id": "E6zJjXc8K8Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(train_ds_raw)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices(val_ds_raw)"
      ],
      "metadata": {
        "id": "9XLkx0b57do4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_and_scale = tf.keras.Sequential(\n",
        "    [\n",
        "     tf.keras.layers.Resizing(224, 224, crop_to_aspect_ratio=True),\n",
        "     tf.keras.layers.Rescaling(1./255)\n",
        "     ]\n",
        ")\n",
        "\n",
        "data_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "     tf.keras.layers.RandomFlip(\"vertical\"),\n",
        "     tf.keras.layers.RandomRotation(0.2),\n",
        "     tf.keras.layers.RandomContrast(0.5),\n",
        "     tf.keras.layers.RandomZoom((-0.2, 0.2)),\n",
        "     tf.keras.layers.RandomTranslation(0.14, 0.14),\n",
        "     ]\n",
        ")\n",
        "\n",
        "def filepath_to_img(filepath):\n",
        "    \n",
        "    img_string = tf.io.read_file(filepath)\n",
        "    img = tf.image.decode_jpeg(img_string)\n",
        "\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "\n",
        "    # if augment:\n",
        "    #     img = data_augmentation(img)\n",
        "\n",
        "    # img = resize_and_scale(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "def configure_dataset(ds, mode):\n",
        "    assert mode == 'training' or mode == 'validation', \"mode should be 'training' or 'validation'\" \n",
        "    \n",
        "    if mode == 'training':\n",
        "        ds = ds.shuffle(buffer_size=20000)\n",
        "    \n",
        "    ds = ds.map(lambda a_path, p_path, n_path: (filepath_to_img(a_path), \n",
        "                                                filepath_to_img(p_path), \n",
        "                                                filepath_to_img(n_path)), \n",
        "                num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.prefetch(buffer_size=500)\n",
        "\n",
        "    # ds.map(lambda a, p, n: (resize_and_scale(a), resize_and_scale(p), resize_and_scale(n)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # if mode == 'validation':        ###############################\n",
        "    ds = ds.cache()\n",
        "\n",
        "    if mode == 'training':\n",
        "        ds = ds.map(lambda a, p, n: (a, \n",
        "                                     data_augmentation(p), \n",
        "                                     n), \n",
        "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    if mode == 'training':\n",
        "        ds = ds.batch(32)\n",
        "    \n",
        "    return ds\n",
        "\n",
        "train_ds = configure_dataset(train_ds, mode='training')\n",
        "val_ds = configure_dataset(val_ds, mode='validation')\n"
      ],
      "metadata": {
        "id": "uJ7Bq0GUMCP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize some triplets\n",
        "\n",
        "ncols = 3\n",
        "nrows = 3\n",
        "plt.figure(figsize=(4*ncols, 4*nrows))\n",
        "spec = gridspec.GridSpec(ncols=ncols, nrows=nrows)\n",
        "for anchor_batch, positive_batch, negative_batch in train_ds.take(1):\n",
        "    i = 0\n",
        "    for anchor, positive, negative in zip(anchor_batch, positive_batch, negative_batch):\n",
        "        plt.subplot(spec[ncols*i])\n",
        "        plt.imshow(anchor)\n",
        "        plt.axis('off')\n",
        "        plt.subplot(spec[ncols*i + 1])\n",
        "        plt.imshow(positive)\n",
        "        plt.axis('off')\n",
        "        plt.subplot(spec[ncols*i + 2])\n",
        "        plt.imshow(negative)\n",
        "        plt.axis('off')\n",
        "        \n",
        "        i += 1\n",
        "        if i == nrows:\n",
        "            break\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C_Ke23HBFK-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# shutil.rmtree(str(temp_dir), ignore_errors=True)"
      ],
      "metadata": {
        "id": "zkU51fPtt8h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup model"
      ],
      "metadata": {
        "id": "KILeLz9iOWAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV3Large(weights='imagenet', input_shape=(224, 224, 3), include_top=False)\n",
        "\n",
        "output = layers.Flatten()(base_model.output)\n",
        "output = layers.Dense(512, activation='relu')(output)\n",
        "output = layers.BatchNormalization()(output)\n",
        "output = layers.Dense(256, activation='relu')(output)\n",
        "output = layers.BatchNormalization()(output)\n",
        "output = layers.Dense(256, activation='relu')(output)\n",
        "output = layers.BatchNormalization()(output)\n",
        "\n",
        "embedding = Model(base_model.input, output, name='Embedding')\n",
        "\n",
        "trainable = False\n",
        "for layer in base_model.layers:\n",
        "    if layer.name == 'multiply_15':\n",
        "        trainable = True\n",
        "    layer.trainable = trainable\n",
        "\n",
        "class Distance(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, anchor, positive, negative):\n",
        "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
        "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
        "        return ap_distance, an_distance\n",
        "\n",
        "anchor_input = layers.Input(name='anchor', shape=(224, 224, 3))\n",
        "positive_input = layers.Input(name='positive', shape=(224, 224, 3))\n",
        "negative_input = layers.Input(name='negative', shape=(224, 224, 3))\n",
        "\n",
        "distances = Distance()(\n",
        "    embedding(mobilenet_v3.preprocess_input(anchor_input)),\n",
        "    embedding(mobilenet_v3.preprocess_input(positive_input)),\n",
        "    embedding(mobilenet_v3.preprocess_input(negative_input))\n",
        ")\n",
        "\n",
        "siamese_base = Model(\n",
        "    inputs=[anchor_input, positive_input, negative_input],\n",
        "    outputs=distances\n",
        ")\n",
        "\n",
        "class SiameseNet(Model):\n",
        "    \"\"\"\n",
        "    Siamese network with triplet loss:\n",
        "        L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
        "    \"\"\"\n",
        "    def __init__(self, siamese_base, margin=0.5):\n",
        "        super(SiameseNet, self).__init__()\n",
        "        self.siamese_base = siamese_base\n",
        "        self.margin = margin\n",
        "        self.loss_tracker = metrics.Mean(name='loss')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.siamese_base(inputs)\n",
        "\n",
        "    @tf.function\n",
        "    def _compute_loss(self, data):\n",
        "        ap_distance, an_distance = self.siamese_base(data)\n",
        "        loss = tf.maximum(ap_distance + self.margin - an_distance, 0.0)\n",
        "        return loss\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self._compute_loss(data)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.siamese_base.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.siamese_base.trainable_weights))\n",
        "        \n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {'loss': self.loss_tracker.result()}\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(self, data):\n",
        "        loss = self._compute_loss(data)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {'loss': self.loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # list our metrics here so the `reset_states()` can be called automatically.\n",
        "        return [self.loss_tracker]\n",
        "\n"
      ],
      "metadata": {
        "id": "JeaSzkDi_x-O"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SiameseNet(siamese_base)\n",
        "model.compile(optimizer=SGD(learning_rate=1e-5, momentum=0.9, nesterov=True))\n",
        "model.fit(train_ds, epochs=2, validation_data=val_ds, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF3xflRKG4CD",
        "outputId": "35a39d17-9835-44a7-8899-37f173b44d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "   45/68775 [..............................] - ETA: 26:58:21 - loss: 0.4866"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = {}\n",
        "epoch_record = {}\n",
        "histories = {}"
      ],
      "metadata": {
        "id": "9Xynu97nIgY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_model(name):\n",
        "    try:\n",
        "        del models[name]\n",
        "        del histories[name]\n",
        "        del epoch_record[name]\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "def train_model(model, optimizer, max_epochs, reduce_lr_patience, early_stop_patience, checkpoint_name):\n",
        "\n",
        "    callbacks = [\n",
        "                 tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=reduce_lr_patience, verbose=1),\n",
        "                 tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stop_patience, verbose=1),\n",
        "                #  tf.keras.callbacks.TensorBoard(logdir/model.name)\n",
        "                ]\n",
        "    if checkpoint_name is not None:\n",
        "        callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/MyDrive/checkpoint/{}.hdf5'.format(checkpoint_name), monitor='val_loss', verbose=1, save_best_only=True))\n",
        "    \n",
        "    # shutil.rmtree(logdir/name, ignore_errors=True)     \n",
        "\n",
        "    new_model = True\n",
        "    if model.name in histories.keys() and input(\"'{}' already exists. Override? [y/n] \".format(model.name)) != 'y':\n",
        "        new_model = False\n",
        "    \n",
        "    set_seed()\n",
        "\n",
        "    if new_model:\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "        )\n",
        "        histories[model.name] = model.fit(\n",
        "            train_ds,\n",
        "            steps_per_epoch=train_ds.cardinality().numpy(),     ############### <----- numpy() to avoid error\n",
        "            epochs=max_epochs,\n",
        "            validation_data=val_ds,\n",
        "            callbacks=callbacks,\n",
        "            shuffle=True\n",
        "        )\n",
        "        epoch_record[model.name] = [histories[model.name].epoch[-1]]\n",
        "\n",
        "    else:\n",
        "        model.optimizer.learning_rate.assign(optimizer.get_config()['learning_rate'])\n",
        "        cont_history = model.fit(\n",
        "            train_ds, \n",
        "            steps_per_epoch=train_ds.cardinality().numpy(),     ############### <----- numpy() to avoid error\n",
        "            epochs=max_epochs,     \n",
        "            validation_data=val_ds,    \n",
        "            callbacks=callbacks,\n",
        "            initial_epoch=histories[model.name].epoch[-1] + 1,\n",
        "            shuffle=True\n",
        "        )\n",
        "        epoch_record[model.name].append(cont_history.epoch[-1])\n",
        "        histories[model.name].epoch += cont_history.epoch\n",
        "        histories[model.name].history['loss'] += cont_history.history['loss']\n",
        "        histories[model.name].history['val_loss'] += cont_history.history['val_loss']\n",
        "\n",
        "    return model.name\n",
        "\n",
        "\n",
        "plotter = tfdocs.plots.HistoryPlotter( \n",
        "    # metric = 'nonregu_mae',\n",
        "    # smoothing_std=10\n",
        ")"
      ],
      "metadata": {
        "id": "MFMDXMUrIkRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depricated"
      ],
      "metadata": {
        "id": "rsW7TaWP6fEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Depricated\n",
        "\n",
        "min_cardinality = 4\n",
        "max_cardinality = label_count.max()         # 39 in this dataset\n",
        "augmented_list = []\n",
        "\n",
        "progress = 0\n",
        "for i in range(len(train_df)):\n",
        "    # ten file: train_df.iloc[i]['image']\n",
        "    # label_count[train_df.iloc[i]['label_group']]\n",
        "    label_group = train_df.iloc[i]['label_group']\n",
        "    class_cardinality = label_count[label_group]\n",
        "    if class_cardinality < min_cardinality:\n",
        "        progress += 1\n",
        "        print(progress)\n",
        "        img = image.load_img(train_img_dir/train_df.iloc[i]['image'])\n",
        "        img = image.img_to_array(img)\n",
        "        num_augmentation = math.ceil((min_cardinality - class_cardinality) / class_cardinality)\n",
        "        for _ in range(num_augmentation):\n",
        "            name = train_df.iloc[i]['image'][:-4] + '_AUGMENTED_' + str(_) + '.jpg'\n",
        "            augmented_img = data_augmentation(img)\n",
        "            image.save_img(train_img_dir/name, augmented_img)\n",
        "            augmented_list.append({'image': name, 'label_group': label_group})\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FKSS-y7cVoQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_train_df = train_df.append(augmented_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "tQb-GkphxQ8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plot class cardinality distribution after augmentation\n",
        "\n",
        "# calculate number of images in each class\n",
        "augmented_label_count = augmented_train_df['label_group'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "augmented_label_count.value_counts().plot.bar()\n",
        "plt.xlabel('Class cardinality')\n",
        "plt.ylabel('Number of class')\n",
        "plt.title('Shopee Class cardinality distribution after augmented')\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHsV_n0LIuUa",
        "outputId": "f8e71c5b-01d0-40aa-b098-33dba21fce79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}